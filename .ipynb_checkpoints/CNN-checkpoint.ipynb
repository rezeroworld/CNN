{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal of this notebook is to show my approach for the Kaggle data competition and some tricks I used to achieve a good rank. We used $1500$ $28 \\times 28$ images to predict the expected class of $60000$ images (there are $6$ classes). I went through a lot of experimentation but I will summarize the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pytorch_cnn_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "I did not spend too much time in the data preparation eventhough it is an important part, I mainly normalized the images and as a validation technique I just kept $30%$ of the data aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the train data\n",
    "train = np.load('data/train.npz')\n",
    "X_train, X_val, y_train, y_val = train_test_split(train['arr_0'], train['arr_1'], test_size=0.3, random_state=42, stratify=train['arr_1'])\n",
    "mean = (X_train / 255.).mean()\n",
    "std = (X_train / 255.).std()\n",
    "\n",
    "# Reshape \n",
    "X_train = X_train.reshape(-1,28,28).astype(int)\n",
    "X_val = X_val.reshape(-1,28,28).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization\n",
    "We use the methods implemented in pytorch_cnn_2 to initialize the model we use, I suggest you check it as it is really a very complete function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have) and val/testing\n",
    "batch_size = 16\n",
    "val_test_batch_size = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = pytorch_cnn_2.initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an optimizer and a loss function\n",
    "The model training and generalization can be greatly affected by the choice of optimizer, after trying SGD, Adam and Adagram the first one was the best performing so I will keep it. For the loss function the most simple one to use for multiclass classification is the Cross Entropy loss, other ones require one hot encoding I believe but anyways we will stick with this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define a scheduler in case you want a dynamic learnign rate\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='max', patience=10, factor=0.7)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading and preprocessing\n",
    "Pytorch offers a complete and simple data preprocessing methods through torchvision.transforms and the simplest way to get access to it is through Dataloader class which takes Dataset class as an input, the problem is that the nature of our dataset makes it difficult for uss to use this tools. The easiest way to do this is to transform our data to images and save them, one elegant way I found to do things is to create a custom Dataset class, the idea is not mine and link is in the readme file. I will not get into the details of preprocessing as it is wildely available and easy to search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing we will apply\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([mean, mean, mean], [std, std, std])\n",
    "    ])\n",
    "    \n",
    "val_transforms = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([mean, mean, mean], [std, std, std])\n",
    "    ])\n",
    "    \n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([mean, mean, mean], [std, std, std])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that inherit the Dataset class\n",
    "class DatasetDraws(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        if y.all() != None:\n",
    "            self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.fromarray(self.X[index]).convert('RGB')\n",
    "        label = self.y[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Dataset\n",
    "train_dataset = DatasetDraws(X_train, y_train, transform=train_transforms)\n",
    "val_dataset = DatasetDraws(X_val, y_val, transform=val_transforms)\n",
    "\n",
    "# torch Dataloaders\n",
    "train_load = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_load = torch.utils.data.DataLoader(val_dataset, batch_size = val_test_batch_size, shuffle = True)\n",
    "dataloaders = {'train': train_load, 'val':val_load}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "The training function available in pytorch_cnn.py is really complete, it includes transfere learning (feature extraction and use of pretrained weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.5831 Acc: 0.3457\n",
      "val Loss: 1.1242 Acc: 0.5800\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.1277 Acc: 0.5781\n",
      "val Loss: 0.6870 Acc: 0.7556\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.9463 Acc: 0.6495\n",
      "val Loss: 0.6613 Acc: 0.7711\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.9498 Acc: 0.6562\n",
      "val Loss: 0.5852 Acc: 0.8044\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.8045 Acc: 0.7000\n",
      "val Loss: 0.5566 Acc: 0.8022\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.8127 Acc: 0.7076\n",
      "val Loss: 0.5620 Acc: 0.8089\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.7411 Acc: 0.7276\n",
      "val Loss: 0.5602 Acc: 0.7933\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.7545 Acc: 0.7219\n",
      "val Loss: 0.4767 Acc: 0.8511\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.7321 Acc: 0.7476\n",
      "val Loss: 0.4700 Acc: 0.8422\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6762 Acc: 0.7524\n",
      "val Loss: 0.5030 Acc: 0.8267\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.6832 Acc: 0.7533\n",
      "val Loss: 0.4317 Acc: 0.8467\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.7781\n",
      "val Loss: 0.4154 Acc: 0.8622\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.5832 Acc: 0.7886\n",
      "val Loss: 0.4284 Acc: 0.8622\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.7848\n",
      "val Loss: 0.4147 Acc: 0.8533\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.7581\n",
      "val Loss: 0.4638 Acc: 0.8400\n",
      "Training complete in 6m 7s\n",
      "Best val Acc: 0.862222\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = pytorch_cnn_2.train_model(model_ft, dataloaders, device, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still the problem of the $30\\%$ data left out for validation for which I came up with an idea that did not result in great outcome, basically after finishing the training we can use the all the dataset to run another training epoch with a small learning rate to prevent the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best approach\n",
    "My best approach yet consisted on ensembling three CNNs, I found the model on the pytorch forum and the link is in the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, modelA, modelB, modelC, nb_classes=6):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        \n",
    "        # Remove last linear layer\n",
    "        self.modelA.fc = nn.Identity()\n",
    "        self.modelB.fc = nn.Identity()\n",
    "        self.modelC.fc = nn.Identity()\n",
    "        \n",
    "        # Create new classifier\n",
    "        self.classifier = nn.Linear(18, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = self.modelB(x.clone())\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x3 = self.modelC(x.clone())\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.classifier(F.relu(x))\n",
    "        return x\n",
    "\n",
    "# Initialize three CNNs\n",
    "modelA, input_size = pytorch_cnn_2.initialize_model('resnet', 6, False, use_pretrained=True)\n",
    "modelB, input_size = pytorch_cnn_2.initialize_model('resnet', 6, False, use_pretrained=True)\n",
    "modelC, input_size = pytorch_cnn_2.initialize_model('resnet', 6, False, use_pretrained=True)\n",
    "\n",
    "# Create ensemble model    \n",
    "model = MyEnsemble(modelA, modelB, modelC, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to send it to GPU and run it. I will not run the model now because I believe it is not the best way to run it. An ensemble consists on training multiple models on different subsets of the data whereas here we are using the same data, I just wanted to show all my approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for improvement\n",
    "I had a bunch of ideas I wanted to try but did not have the time and sometimes the skills. A pretty simple approach was to try the Mixup technique which is a data augmentation technique that uses one hot encoding. Another technique is to use GANs for data augmentation but I am not sure if it will work as I don't have any experience with GANs. Finally, finding a better validation method could surely lead to better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
